# -*- coding: utf-8 -*-
"""TIMSER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BIxMt-JqppCCaK-cD9YSKuKmj4L5e4v2
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import Dropout

train_data = pd.read_csv('/content/train (1).csv')
val_data = pd.read_csv('/content/val.csv')
print(train_data.head())
print(val_data.head())

train_set = train_data.iloc[:,1:2].values
print(train_set[0:5])

sc = MinMaxScaler(feature_range=(0,1))
train_set_scaled = sc.fit_transform(train_set)
print(train_set_scaled[0:5])

print(len(train_set_scaled))

X_train = []
Y_train = []

for i in range(60,len(train_set_scaled)):
  X_train.append(train_set_scaled[i-60:i,0])
  Y_train.append(train_set_scaled[i,0])
X_train = np.array(X_train)
Y_train = np.array(Y_train)
print(X_train.shape)
print(Y_train.shape)

X_train=np.reshape(X_train,(X_train.shape[0],X_train.shape[1],1))
print(X_train.shape)

val_set = val_data.iloc[:,1:2].values
print(val_set[0:5])

val_set_scaled = sc.transform(val_set)
print(val_set_scaled[0:5])

X_val = []
Y_val = []

for i in range(60,len(val_set_scaled)):
  X_val.append(val_set_scaled[i-60:i,0])
  Y_val.append(val_set_scaled[i,0])
X_val = np.array(X_val)
Y_val = np.array(Y_val)
print(X_val.shape)
print(Y_val.shape)

X_val=np.reshape(X_val,(X_val.shape[0],X_val.shape[1],1))
print(X_val.shape)

model = Sequential()
model.add(LSTM(units=50,return_sequences=True,input_shape=(X_train.shape[1],1)))
model.add(Dropout(0.4))
model.add(LSTM(units=50,return_sequences=True))
model.add(Dropout(0.4))
model.add(LSTM(units=50,return_sequences=True))
model.add(Dropout(0.4))
model.add(LSTM(units=50))
model.add(Dropout(0.4))
model.add(Dense(units=1))

model.summary()

model.compile(optimizer='adam',loss='mean_squared_error')
history = model.fit(X_train,Y_train,epochs=100,batch_size=32,validation_data=(X_val, Y_val))

print(history.history.keys())

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

!cp '/content/sample_submission (4).csv' '/content/output.csv'

test_data = pd.read_csv('/content/sample_submission (4).csv')
out_data  = pd.read_csv('/content/output.csv')
test_set = val_data.iloc[len(val_data)-60:,1:2].values
print(len(test_set))
print(test_set.shape)

print(len(test_data))
print(test_data.head())
test_data_date = test_data.iloc[:,0:1].values
print(test_data_date[0:5])

print(out_data['date'][0])

from pickle import dump
# save the model
model.save_weights("model.h5")
print("Saved model to disk")
# save the scaler
dump(sc, open('scaler.pkl', 'wb'))

from pickle import load
model.load_weights("model.h5")
print("Loaded model from disk")
scaler = load(open('scaler.pkl', 'rb'))

test_set_scaled = scaler.transform(test_set)
for i in range(len(test_data_date)):
  print(i)
  #print(test_set_scaled.shape)
  X_test = test_set_scaled[i:60+i,0]
  #print(X_test.shape)
  X_test = np.reshape(X_test,(X_test.shape[0],1))
  X_test = np.expand_dims(X_test,axis=0)
  #print(X_test.shape)
  Y_pred = model.predict(X_test)
  test_set_scaled = np.append(test_set_scaled,Y_pred, axis=0)
  predicted_stock = sc.inverse_transform(Y_pred)
  #out_df.append([test_data_date[i],predicted_stock])
  
  #val = str(test_data_date[i]).lstrip('[').rstrip(']')
  #print(val)
  print(predicted_stock.flatten())
  val = ''.join(test_data_date[i].flatten())
  print(val)
  #out_data['date'][str(val)] =  predicted_stock.flatten()
  out_data.loc[out_data.date == str(val),'value'] = predicted_stock.flatten()

print(out_data.head())

# df = pd.DataFrame(out_data, columns = ['date', 'value']) 
# print(df.head())

out_data.to_csv('file3.csv',index=False)